"""
CORE LIQUIDATION AGGREGATION ENGINE - Production Grade with Type Hints
Multi-Level Storage Architecture: In-Memory ‚Üí Redis ‚Üí TimescaleDB
Supports: Binance + Bybit, BTCUSDT (Phase 1)
Enhanced with comprehensive type annotations for better IDE support and type safety
"""

from __future__ import annotations

import asyncio
import json
import time
import uuid
import logging
import os
from collections import deque
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from typing import (
    Optional, Dict, List, Deque, Any, Tuple, Union,
    TypedDict, Protocol, AsyncGenerator, Awaitable,
    Final, Literal, ClassVar, cast, TYPE_CHECKING
)
from enum import IntEnum

import websockets
from websockets.client import WebSocketClientProtocol
from websockets.exceptions import WebSocketException
import redis.asyncio as redis
from redis import ConnectionPool
import asyncpg
from asyncpg.pool import Pool
from asyncpg.connection import Connection
import numpy as np
from numpy.typing import NDArray

# =============================================================================
# CONFIGURATION
# =============================================================================

# Type definitions for configuration
ExchangeName = Literal['binance', 'bybit', 'okx']
Symbol = str  # Trading symbol like 'BTCUSDT'

# Redis configuration (using 'liq:' prefix to avoid conflicts)
REDIS_HOST: Final[str] = os.getenv('REDIS_HOST', 'localhost')
REDIS_PORT: Final[int] = int(os.getenv('REDIS_PORT', 6380))
REDIS_DB: Final[int] = int(os.getenv('REDIS_LIQ_DB', 1))  # Use DB 1 for liquidations
REDIS_PREFIX: Final[str] = 'liq:'  # All keys prefixed with 'liq:'

# TimescaleDB configuration
DB_HOST: Final[str] = os.getenv('DB_HOST', 'localhost')
DB_PORT: Final[int] = int(os.getenv('DB_PORT', 5432))
DB_NAME: Final[str] = os.getenv('DB_NAME', 'liquidations')
DB_USER: Final[str] = os.getenv('DB_USER', os.getenv('USER', 'postgres'))
DB_PASSWORD: Final[str] = os.getenv('DB_PASSWORD', '')

# Exchange WebSocket URLs
EXCHANGE_URLS: Final[Dict[ExchangeName, str]] = {
    'binance': 'wss://fstream.binance.com/ws/!forceOrder@arr',
    'bybit': 'wss://stream.bybit.com/v5/public/linear'
}

# Symbols to track (Phase 1: BTCUSDT only)
TRACKED_SYMBOLS: Final[List[Symbol]] = ['BTCUSDT']

# Liquidation thresholds (institutional level)
INSTITUTIONAL_THRESHOLD_USD: Final[float] = 100_000  # $100K+ only
CASCADE_MIN_COUNT: Final[int] = 5  # 5+ liquidations in window
CASCADE_WINDOW_SECONDS: Final[int] = 60  # 60-second cascade window

# Price level rounding
PRICE_LEVEL_ROUNDING: Final[Dict[str, int]] = {
    'BTCUSDT': 100,  # Round to $100 levels
    'ETHUSDT': 100,  # Round to $100 levels
    'default': 10    # Round to $10 levels for others
}

# Memory limits
RING_BUFFER_SIZE: Final[int] = 1000  # Keep last 1000 events per symbol

# =============================================================================
# DATA MODELS
# =============================================================================

class Exchange(IntEnum):
    """Exchange enum for compact storage"""
    BINANCE = 0
    BYBIT = 1
    OKX = 2

class Side(IntEnum):
    """Side enum for compact storage"""
    LONG = 0
    SHORT = 1

@dataclass
class LiquidationEvent:
    """
    Compact liquidation event structure
    Total size: ~60 bytes (cache-friendly)
    """
    __slots__ = ['timestamp_ms', 'exchange', 'symbol', 'side',
                 'price', 'quantity', 'value_usd']

    timestamp_ms: int       # Unix timestamp in milliseconds
    exchange: int           # Exchange enum (0=Binance, 1=Bybit)
    symbol: str             # Symbol (BTCUSDT)
    side: int               # Side enum (0=LONG, 1=SHORT)
    price: float            # Price
    quantity: float         # Quantity
    value_usd: float        # Value in USD

    @property
    def timestamp(self) -> datetime:
        """Convert to datetime"""
        return datetime.fromtimestamp(self.timestamp_ms / 1000)

    @property
    def exchange_name(self) -> str:
        """Get exchange name"""
        return Exchange(self.exchange).name.lower()

    @property
    def side_name(self) -> str:
        """Get side name"""
        return 'LONG' if self.side == Side.LONG else 'SHORT'

    def to_dict(self) -> Dict[str, Union[str, float]]:
        """Convert to dictionary"""
        return {
            'timestamp': self.timestamp.isoformat(),
            'exchange': self.exchange_name,
            'symbol': self.symbol,
            'side': self.side_name,
            'price': self.price,
            'quantity': self.quantity,
            'value_usd': self.value_usd
        }


@dataclass
class PriceLevelCluster:
    """Aggregated price level data"""
    price_level: float
    event_count: int
    total_value_usd: float
    total_quantity: float
    long_count: int
    short_count: int
    exchanges: List[str]
    first_seen_ms: int
    last_seen_ms: int


@dataclass
class CascadeEvent:
    """Cascade event data"""
    cascade_id: str
    symbol: str
    start_time_ms: int
    end_time_ms: int
    event_count: int
    total_value_usd: float
    exchanges: List[str]
    risk_score: float
    events: List[LiquidationEvent]


# =============================================================================
# LEVEL 1: IN-MEMORY PROCESSING (Ring Buffers)
# =============================================================================

class InMemoryLiquidationBuffer:
    """
    Ultra-fast in-memory liquidation buffer
    Processing latency: <100 microseconds
    """

    def __init__(self, maxlen: int = RING_BUFFER_SIZE) -> None:
        self.buffers: Dict[Symbol, Deque[LiquidationEvent]] = {}
        self.maxlen: int = maxlen
        self.processed_count: int = 0
        self.start_time: float = time.time()

        # Initialize buffers for tracked symbols
        for symbol in TRACKED_SYMBOLS:
            self.buffers[symbol] = deque(maxlen=maxlen)

    def add_event(self, event: LiquidationEvent) -> None:
        """
        Add event to ring buffer (O(1) operation)
        Latency: ~1 microsecond
        """
        if event.symbol not in self.buffers:
            self.buffers[event.symbol] = deque(maxlen=self.maxlen)

        self.buffers[event.symbol].append(event)
        self.processed_count += 1

    def get_recent_events(self, symbol: Symbol, seconds: int = 60) -> List[LiquidationEvent]:
        """
        Get events from last N seconds
        Latency: ~10-50 microseconds
        """
        if symbol not in self.buffers:
            return []

        cutoff_ms: int = int((time.time() - seconds) * 1000)
        return [e for e in self.buffers[symbol] if e.timestamp_ms >= cutoff_ms]

    def detect_cascade_fast(self, symbol: Symbol, window_seconds: int = CASCADE_WINDOW_SECONDS) -> Optional[List[LiquidationEvent]]:
        """
        Ultra-fast cascade detection
        Latency: ~10-50 microseconds
        """
        recent: List[LiquidationEvent] = self.get_recent_events(symbol, window_seconds)

        if len(recent) >= CASCADE_MIN_COUNT:
            # Check if total value meets threshold
            total_value: float = sum(e.value_usd for e in recent)
            if total_value >= INSTITUTIONAL_THRESHOLD_USD:
                return recent

        return None

    def get_stats(self) -> Dict[str, Union[int, float, Dict[Symbol, int]]]:
        """Get buffer statistics"""
        uptime: float = time.time() - self.start_time
        return {
            'processed_events': self.processed_count,
            'events_per_second': self.processed_count / uptime if uptime > 0 else 0,
            'buffer_sizes': {sym: len(buf) for sym, buf in self.buffers.items()},
            'uptime_seconds': uptime
        }


# =============================================================================
# LEVEL 2: REDIS AGGREGATION (Price Levels + Time Buckets)
# =============================================================================

class RedisLiquidationCache:
    """
    Redis-based aggregation layer
    Write latency: <1 ms
    Read latency: <0.5 ms
    """

    def __init__(self, redis_client: redis.Redis) -> None:
        self.redis: redis.Redis = redis_client
        self.prefix: str = REDIS_PREFIX

    async def cache_price_level(self, event: LiquidationEvent) -> None:
        """
        Cache event in price-level clusters
        Latency: <1 ms
        """
        # Round price to level
        rounding = PRICE_LEVEL_ROUNDING.get(event.symbol, PRICE_LEVEL_ROUNDING['default'])
        price_level = (event.price // rounding) * rounding

        # Key: liq:levels:BTCUSDT:67200:LONG
        key = f"{self.prefix}levels:{event.symbol}:{int(price_level)}:{event.side_name}"

        pipe = self.redis.pipeline()

        # Increment counters
        pipe.hincrby(key, 'count', 1)
        pipe.hincrbyfloat(key, 'total_value', event.value_usd)
        pipe.hincrbyfloat(key, 'total_quantity', event.quantity)

        # Track exchanges
        pipe.sadd(f"{key}:exchanges", event.exchange_name)

        # Set timestamps
        if not await self.redis.hexists(key, 'first_seen'):
            pipe.hset(key, 'first_seen', event.timestamp_ms)
        pipe.hset(key, 'last_seen', event.timestamp_ms)

        # Set TTL (1 hour)
        pipe.expire(key, 3600)

        await pipe.execute()

    async def cache_time_bucket(self, event: LiquidationEvent, bucket_seconds: int = 60) -> None:
        """
        Cache event in time-bucketed aggregations
        Latency: <1 ms
        """
        # Calculate bucket timestamp
        bucket_ts = (event.timestamp_ms // (bucket_seconds * 1000)) * (bucket_seconds * 1000)

        # Key: liq:agg:BTCUSDT:1m:1729512000000
        key = f"{self.prefix}agg:{event.symbol}:{bucket_seconds}s:{bucket_ts}"

        pipe = self.redis.pipeline()

        # Aggregate data
        pipe.hincrby(key, 'count', 1)
        pipe.hincrbyfloat(key, 'total_value', event.value_usd)
        pipe.hincrby(key, f'{event.side_name.lower()}_count', 1)
        pipe.hincrby(key, f'{event.exchange_name}_count', 1)

        # Track if institutional
        if event.value_usd >= INSTITUTIONAL_THRESHOLD_USD:
            pipe.hincrby(key, 'institutional_count', 1)

        # Set TTL (1 hour)
        pipe.expire(key, 3600)

        await pipe.execute()

    async def set_cascade_status(self, cascade: CascadeEvent) -> None:
        """
        Set cascade status flag
        Latency: <0.5 ms
        """
        key = f"{self.prefix}cascade:status:{cascade.symbol}"

        data = {
            'active': True,
            'cascade_id': cascade.cascade_id,
            'start_time': cascade.start_time_ms,
            'event_count': cascade.event_count,
            'total_value': cascade.total_value_usd,
            'risk_score': cascade.risk_score,
            'exchanges': ','.join(cascade.exchanges)
        }

        await self.redis.hset(key, mapping={k: str(v) for k, v in data.items()})
        await self.redis.expire(key, 300)  # 5 minutes TTL

    async def get_price_level_clusters(self, symbol: str, min_count: int = 2) -> List[PriceLevelCluster]:
        """
        Get price level clusters
        Latency: <10 ms
        """
        pattern = f"{self.prefix}levels:{symbol}:*"
        clusters = []

        async for key in self.redis.scan_iter(match=pattern):
            data = await self.redis.hgetall(key)
            if not data:
                continue

            count = int(data.get(b'count', 0))
            if count < min_count:
                continue

            # Parse key to get price level and side
            parts = key.decode().split(':')
            price_level = float(parts[2])
            side = parts[3]

            # Get exchanges
            exchanges_key = f"{key.decode()}:exchanges"
            exchanges = await self.redis.smembers(exchanges_key)
            exchanges_list = [e.decode() for e in exchanges]

            cluster = PriceLevelCluster(
                price_level=price_level,
                event_count=count,
                total_value_usd=float(data.get(b'total_value', 0)),
                total_quantity=float(data.get(b'total_quantity', 0)),
                long_count=count if side == 'LONG' else 0,
                short_count=count if side == 'SHORT' else 0,
                exchanges=exchanges_list,
                first_seen_ms=int(data.get(b'first_seen', 0)),
                last_seen_ms=int(data.get(b'last_seen', 0))
            )
            clusters.append(cluster)

        # Sort by event count descending
        clusters.sort(key=lambda x: x.event_count, reverse=True)
        return clusters

    async def get_cascade_status(self, symbol: str) -> Optional[Dict]:
        """
        Get current cascade status
        Latency: <0.5 ms
        """
        key = f"{self.prefix}cascade:status:{symbol}"
        data = await self.redis.hgetall(key)

        if not data:
            return None

        return {k.decode(): v.decode() for k, v in data.items()}


# =============================================================================
# LEVEL 3: TIMESCALEDB ASYNC WRITER
# =============================================================================

class AsyncDatabaseWriter:
    """
    Async database writer for TimescaleDB
    Writes in background, never blocks real-time processing
    """

    def __init__(self) -> None:
        self.queue: asyncio.Queue[Tuple[LiquidationEvent, Optional[str], Optional[float]]] = asyncio.Queue(maxsize=100_000)
        self.db_pool: Optional[Pool] = None
        self.running: bool = False
        self.written_count: int = 0

    async def init_db(self) -> None:
        """Initialize database connection pool"""
        self.db_pool = await asyncpg.create_pool(
            host=DB_HOST,
            port=DB_PORT,
            database=DB_NAME,
            user=DB_USER,
            password=DB_PASSWORD,
            min_size=2,
            max_size=10,
            command_timeout=60
        )
        logging.info("‚úÖ Database connection pool initialized")

    def queue_event(self, event: LiquidationEvent, cascade_id: Optional[str] = None, risk_score: Optional[float] = None) -> None:
        """
        Queue ALL events for async writing (non-blocking)
        Latency: <1 microsecond
        """
        try:
            self.queue.put_nowait((event, cascade_id, risk_score))
        except asyncio.QueueFull:
            logging.warning("‚ö†Ô∏è  DB queue full, dropping event")

    async def background_writer(self):
        """
        Background task that writes to database
        Batches every 10 seconds or 1000 events
        """
        self.running = True
        batch = []
        last_write = time.time()

        logging.info("üöÄ Background database writer started")

        while self.running:
            try:
                # Collect events for batch
                while len(batch) < 1000:
                    try:
                        item = await asyncio.wait_for(self.queue.get(), timeout=0.1)
                        batch.append(item)
                    except asyncio.TimeoutError:
                        break

                # Write batch if conditions met
                if (time.time() - last_write >= 10 or len(batch) >= 1000) and batch:
                    await self._write_batch(batch)
                    batch.clear()
                    last_write = time.time()

            except Exception as e:
                logging.error(f"‚ùå DB writer error: {e}")
                await asyncio.sleep(1)

    async def _write_batch(self, batch: List[tuple]) -> None:
        """
        Bulk insert events to TimescaleDB
        Performance: 1000 events in ~50-100ms
        """
        if not batch or not self.db_pool:
            return

        try:
            # Prepare data for INSERT
            records = []
            for event, cascade_id, risk_score in batch:
                records.append((
                    event.timestamp,
                    event.exchange_name,
                    event.symbol,
                    event.side_name,
                    event.price,
                    event.quantity,
                    event.value_usd,
                    cascade_id is not None,
                    uuid.UUID(cascade_id) if cascade_id else None,
                    None,  # cascade_event_count (can be calculated)
                    risk_score,
                    None,  # session_type (can be calculated)
                    json.dumps(event.to_dict())
                ))

            # Bulk INSERT
            async with self.db_pool.acquire() as conn:
                await conn.executemany(
                    """
                    INSERT INTO liquidations_significant (
                        time, exchange, symbol, side, price, quantity, value_usd,
                        is_cascade, cascade_id, cascade_event_count, risk_score,
                        session_type, raw_data
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                    ON CONFLICT (time, exchange, symbol, side) DO NOTHING
                    """,
                    records
                )

            self.written_count += len(batch)
            logging.info(f"‚úÖ Wrote {len(batch)} events to TimescaleDB (total: {self.written_count})")

        except Exception as e:
            logging.error(f"‚ùå Batch write failed: {e}")

    async def close(self):
        """Close database connection pool"""
        self.running = False
        if self.db_pool:
            await self.db_pool.close()
            logging.info("Database connection pool closed")


# Continue in next message due to length...
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
