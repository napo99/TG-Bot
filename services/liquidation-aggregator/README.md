# ğŸš€ Multi-Exchange Liquidation Aggregator

**Phase 1:** Binance + Bybit | BTCUSDT & ETHUSDT | Multi-Level Storage Architecture

## ğŸ“‹ Overview

Real-time liquidation tracking system with intelligent multi-level data storage:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LEVEL 1: In-Memory Ring Buffers (<100 Âµs latency)         â”‚
â”‚  â€¢ Last 60 seconds of events                                â”‚
â”‚  â€¢ Ultra-fast cascade detection                             â”‚
â”‚  â€¢ ~18KB per symbol                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LEVEL 2: Redis Aggregation (<1 ms latency)                â”‚
â”‚  â€¢ Price-level clusters ($100 levels for BTC)               â”‚
â”‚  â€¢ Time-bucketed aggregations (1-min buckets)               â”‚
â”‚  â€¢ Cascade status flags                                     â”‚
â”‚  â€¢ TTL: 1 hour auto-expire                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LEVEL 3: TimescaleDB Significant Events (50-100ms batch)  â”‚
â”‚  â€¢ Only institutional liquidations (>$100K)                 â”‚
â”‚  â€¢ Cascade events with risk scoring                         â”‚
â”‚  â€¢ Compression: 10x after 7 days                            â”‚
â”‚  â€¢ Retention: 90 days auto-delete                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LEVEL 4: TimescaleDB Continuous Aggregates                â”‚
â”‚  â€¢ Hourly rollups                                           â”‚
â”‚  â€¢ Price-level heatmaps                                     â”‚
â”‚  â€¢ Exchange comparison                                      â”‚
â”‚  â€¢ Cascade analysis                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Features

### **Core Capabilities:**
- âœ… **Multi-Exchange Support:** Binance + Bybit (Phase 1)
- âœ… **Real-Time Processing:** <100 microsecond hot path
- âœ… **Price-Level Aggregation:** $100 levels for BTC
- âœ… **Cross-Exchange Cascade Detection:** 6-factor risk analysis
- âœ… **Intelligent Storage:** Only meaningful events stored
- âœ… **Auto-Compression:** 10x space savings after 7 days
- âœ… **Auto-Retention:** 90-day auto-delete policy

### **Intelligence:**
- **6-Factor Cascade Risk Scoring:**
  1. Volume concentration
  2. Time compression (events per minute)
  3. Price clustering
  4. Side imbalance
  5. Institutional ratio
  6. Cross-exchange correlation

- **Institutional Filtering:**
  - Only tracks liquidations â‰¥$100K
  - Eliminates 99.9% of retail noise
  - Focuses on market-moving events

---

## ğŸ› ï¸ Setup Instructions

### **Prerequisites:**
- Python 3.11+
- Redis
- PostgreSQL 17 + TimescaleDB 2.22+

### **1. Install Dependencies**

```bash
cd /Users/screener-m3/projects/crypto-assistant/services/liquidation-aggregator

# Install Python dependencies
pip install -r requirements.txt
```

### **2. Verify Database Setup**

TimescaleDB should already be configured. Verify:

```bash
# Check PostgreSQL is running
brew services list | grep postgresql

# Verify TimescaleDB extension
psql liquidations -c "SELECT extname, extversion FROM pg_extension WHERE extname = 'timescaledb';"
```

Expected output:
```
  extname   | extversion
------------+------------
 timescaledb | 2.22.1
```

### **3. Verify Redis**

```bash
# Check Redis is running
brew services list | grep redis

# Test Redis connection
redis-cli -h localhost -p 6380 ping
# Should return: PONG
```

### **4. Environment Configuration (Optional)**

Create `.env` file if you need custom configuration:

```bash
# Redis configuration
REDIS_HOST=localhost
REDIS_PORT=6380
REDIS_LIQ_DB=1  # Use DB 1 for liquidations (avoids conflicts)

# TimescaleDB configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=liquidations
DB_USER=screener-m3
DB_PASSWORD=

# Tracking configuration
TRACKED_SYMBOLS=BTCUSDT,ETHUSDT
INSTITUTIONAL_THRESHOLD_USD=100000
```

### **5. Operational Scripts**

All operator-facing entry points live under `scripts/`. See `scripts/README.md` for full usage notes. Quick examples:

```bash
# HyperLiquid live dashboard
python -m scripts.run_liquidation_monitor

# Professional multi-exchange monitor (Redis defaults to 6380)
python -m scripts.run_professional_monitor --symbols BTCUSDT ETHUSDT --exchanges binance hyperliquid
# Add --full-dashboard to restore the legacy full-screen layout

# Market context (open interest / funding snapshot)
python -m scripts.show_market_context --symbol BTCUSDT --once
```

---

## ğŸ§  Unified Open Interest Engine (OctoberÂ 2025 refresh)

The multi-exchange OI implementation has been consolidated under `src/core/exchanges`. Production wrappers remain in `services/market-data/` so existing tooling keeps working, but new integrations should import directly from the `core.exchanges` package.

- **Library usage**
  ```python
  from core.exchanges.unified_aggregator import UnifiedOIAggregator
  ```
  Each provider (Binance, Bybit, OKX, Gate.io, Bitget, Hyperliquid) lives alongside the aggregator in the same package.

- **One-off snapshot**
  ```bash
  # Uses the compatibility wrapper which forwards to the new core package
  python services/market-data/unified_oi_aggregator.py
  ```
  This writes a fresh `unified_oi_results.json` at the repository root and logs per-exchange health. The command requires only Python and internet access; no Redis/Postgres services are needed.

- **Automated validation**
  ```bash
  pytest services/liquidation-aggregator/tests/test_velocity_engine.py \
         services/liquidation-aggregator/tests/test_signal_generation.py
  ```
  These suites cover the downstream consumers that rely on the OI totals and ensure the refactor didnâ€™t break risk calculations.

---

## ğŸ’¥ HyperLiquid Liquidation Monitor (Dynamic vault tracking)

HyperLiquid liquidations no longer rely on a single hard-coded wallet. The provider now watches the public trade feed and cross-references `userFills` from the HLP vault via the registry in `dex/hyperliquid_liquidation_registry.py`.

- **Run the live dashboard**
  ```bash
  python monitor_liquidations_live.py
  ```
  The CLI refreshes in place, showing live trades, liquidation counts, and registry cache status. Expect a â€œRegistry cacheâ€ line with the latest fill timestamp; if it stops updating, investigate API access.

- **Integrate with professional monitor**
  `professional_liquidation_monitor.py` imports the same provider, so multi-exchange runs automatically benefit from the dynamic detection.

- **Regression tests**
  ```bash
  pytest services/liquidation-aggregator/tests/test_hyperliquid_registry.py
  ```
  This mocks the HyperLiquid API responses and verifies that `tid` matching, side classification, and cache refresh behave as expected. Running the full suite (`pytest services/liquidation-aggregator`) is recommended before deployments.

---

## ğŸš€ Running the Aggregator

### **Start the Aggregator:**

```bash
cd /Users/screener-m3/projects/crypto-assistant/services/liquidation-aggregator

# Run the aggregator
python main.py
```

Expected output:
```
================================================================================
LIQUIDATION AGGREGATOR - PHASE 1
Exchanges: Binance + Bybit | Symbols: BTCUSDT, ETHUSDT
Multi-Level Storage: In-Memory â†’ Redis â†’ TimescaleDB
================================================================================
2025-10-21 10:30:00 - main - INFO - ğŸš€ Initializing Liquidation Aggregator...
2025-10-21 10:30:00 - main - INFO - âœ… Redis connected (DB 1, prefix: liq:)
2025-10-21 10:30:00 - main - INFO - âœ… TimescaleDB connected
2025-10-21 10:30:00 - main - INFO - âœ… Background database writer started
2025-10-21 10:30:00 - main - INFO - ğŸ¯ Liquidation Aggregator ready!
2025-10-21 10:30:00 - aggregator - INFO - Starting 2 exchange streams...
2025-10-21 10:30:01 - binance - INFO - âœ… Connected to Binance liquidation stream
2025-10-21 10:30:01 - bybit - INFO - âœ… Connected to Bybit liquidation stream
2025-10-21 10:30:01 - bybit - INFO - Subscribed to Bybit liquidations for BTCUSDT
```

### **Monitor Real-Time Events:**

When liquidations occur, you'll see:
```
2025-10-21 10:35:15 - main - INFO - ğŸ’° INSTITUTIONAL: BINANCE BTCUSDT LONG $168,086 @ $67,234.50
2025-10-21 10:35:20 - cascade_detector - WARNING - ğŸš¨ CROSS-EXCHANGE CASCADE DETECTED: BTCUSDT | 7 liquidations | $875,200 | Risk: 1.45 | Exchanges: binance, bybit
2025-10-21 10:36:00 - main - INFO - ğŸ“Š STATS: Processed: 145 | Institutional: 12 | Cascades: 2 | Events/sec: 2.42 | Price levels: 8 | DB writes: 12
```

---

## ğŸ“Š Data Analysis & Visualization

### **Option 1: Real-Time Terminal Dashboard (NEW!)**

```bash
# Visual real-time monitoring in terminal
python3 visual_monitor.py
```

Shows **live updating charts** in your terminal:
- â±ï¸ **Timeline**: 10-minute buckets showing volume evolution
- ğŸ“Š **Latest 10 liquidations**: Real-time event stream with timestamps
- ğŸ¦ **Exchange comparison**: Bar charts comparing Binance vs Bybit
- ğŸ“ˆ **Statistics**: Total events, USD volume, cascade count

**Updates every 10 seconds** with beautiful colored terminal output!

---

### **Option 2: Interactive Jupyter Notebooks**

**A) Time-Series Visual Analysis (NEW!):**
```bash
# Beautiful interactive charts showing evolution over time
jupyter notebook analysis_visual.ipynb
```

Provides **7 interactive visualizations**:
1. ğŸ’° **Volume Evolution**: Line charts showing liquidation volume over time
2. ğŸ”¥ **Daily Heatmap**: When liquidations happen (day Ã— hour)
3. âš¡ **Cascade Timeline**: Scatter plot of cascade events with risk scores
4. ğŸ“‰ **Long vs Short**: Comparison of liquidation sides over time
5. ğŸ¦ **Market Share**: Exchange dominance evolution
6. ğŸ“… **Daily Summary**: Multi-panel view (USD, BTC, event count)
7. ğŸ“ˆ **Cumulative Volume**: Total liquidations accumulated over time

**B) Data Analysis (Original):**
```bash
# Detailed data exploration
jupyter notebook analysis.ipynb
```

Provides:
- Price-level heatmaps
- Cascade analysis
- Exchange comparison
- Time series visualizations
- Real-time cascade status

---

### **Option 3: Direct Database Queries**

```bash
# Connect to database
psql liquidations

# Get recent significant liquidations
SELECT time, exchange, symbol, side, value_usd, is_cascade, risk_score
FROM liquidations_significant
WHERE time >= NOW() - INTERVAL '1 hour'
ORDER BY value_usd DESC
LIMIT 10;

# Get price level clusters
SELECT * FROM get_price_level_clusters('BTCUSDT', 4);

# Get hourly rollups
SELECT * FROM liquidations_hourly
WHERE hour >= NOW() - INTERVAL '24 hours'
ORDER BY hour DESC;

# Get cascade events
SELECT * FROM liquidation_cascades
WHERE hour >= NOW() - INTERVAL '24 hours'
ORDER BY total_value_usd DESC;
```

### **Option 3: Redis Inspection**

```bash
# Connect to Redis
redis-cli -n 1

# List all liquidation keys
KEYS liq:*

# Get price level cluster
HGETALL liq:levels:BTCUSDT:67200:LONG

# Get time bucket aggregation
HGETALL liq:agg:BTCUSDT:60s:1729512000000

# Get cascade status
HGETALL liq:cascade:status:BTCUSDT
```

---

## ğŸ¯ What Gets Stored

### **Level 1 (In-Memory):**
- **All events** (last 60 seconds only)
- Automatically dropped after 60s
- Used for real-time cascade detection

### **Level 2 (Redis):**
- **Price-level clusters** (e.g., "15 liquidations at $67,200 level")
- **Time-bucketed aggregations** (1-minute summaries)
- **Cascade status flags**
- TTL: 1 hour (auto-expire)

### **Level 3 (TimescaleDB Significant Events):**
- **Only institutional liquidations** (â‰¥$100K)
- **Cascade events** with full context
- Compressed after 7 days (10x savings)
- Auto-deleted after 90 days

### **Level 4 (TimescaleDB Continuous Aggregates):**
- **Hourly rollups** (pre-computed summaries)
- **Price-level heatmaps** (where liquidations cluster)
- **Exchange comparison** (volume breakdown)
- **Cascade analysis** (cascade patterns)
- Retention: 1 year

---

## ğŸ“ˆ Storage Estimates

**For BTCUSDT with typical liquidation activity:**

| Level | Storage | Retention | Notes |
|-------|---------|-----------|-------|
| L1: In-Memory | ~18 KB | 60 seconds | Ring buffer, automatic |
| L2: Redis | ~1-5 MB | 1 hour | TTL auto-expire |
| L3: Significant Events | ~100 MB/month | 90 days | Compressed 10x |
| L4: Continuous Aggregates | ~10 MB/month | 1 year | Pre-computed |

**Result:** ~99.9% storage reduction vs storing all events

---

## ğŸ”§ Troubleshooting

### **Issue: Redis connection refused**
```bash
# Start Redis
brew services start redis

# Verify it's running
redis-cli ping
```

### **Issue: Database connection error**
```bash
# Check PostgreSQL is running
brew services list | grep postgresql

# Restart if needed
brew services restart postgresql@17

# Verify TimescaleDB extension
psql liquidations -c "SELECT extname FROM pg_extension;"
```

### **Issue: No data appearing**
- Check aggregator is running: `ps aux | grep main.py`
- Check logs: `tail -f liquidations.log`
- Verify WebSocket connections are active (check logs for "Connected to...")
- Check if BTCUSDT is actively trading (may be low liquidation periods)

### **Issue: "Module not found" errors**
```bash
# Make sure you're in the correct directory
cd /Users/screener-m3/projects/crypto-assistant/services/liquidation-aggregator

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ“ Understanding the Data

### **What is a "Price Level"?**
Price levels are rounded price points where liquidations cluster. For BTC, we round to $100 levels.

Example:
- Liquidations at $67,234, $67,245, $67,289 â†’ All counted at $67,200 level
- This reveals "support/resistance" levels where leverage is concentrated

### **What is a "Cascade"?**
A cascade occurs when:
1. **5+ liquidations** in 60 seconds
2. **Total value â‰¥$100K**
3. Often triggers more liquidations as price moves

**Cross-exchange cascades** (occurring on both Binance AND Bybit simultaneously) indicate systemic risk.

### **What is the "Risk Score"?**
A composite score (0.0 to 2.0+) combining:
- Volume concentration (how much is being liquidated)
- Time compression (how fast)
- Price clustering (how focused)
- Side imbalance (all longs or all shorts)
- Institutional ratio (large liquidations)
- Cross-exchange correlation (systemic risk)

**Risk Score Interpretation:**
- `0.0 - 0.7`: Low risk, normal liquidations
- `0.7 - 1.0`: Moderate risk, watch for continuation
- `1.0 - 1.5`: High risk, cascade likely
- `1.5 - 2.0+`: Extreme risk, major cascade event

---

## ğŸ“‹ Phase 2 Roadmap

- [ ] Add ETHUSDT tracking
- [ ] Add SOLUSDT tracking
- [ ] Expand to 3 more exchanges (Bitfinex, Bitmex, Gate.io)
- [ ] Add 7 more symbols (BNB, ARB, MATIC, AVAX, OP, LINK, UNI)
- [ ] Web dashboard (optional)
- [ ] Machine learning cascade prediction (optional)

---

## ğŸ¤ Questions?

For issues or questions:
1. Check logs: `tail -f liquidations.log`
2. Verify database schema: `psql liquidations -c "\d liquidations_significant"`
3. Check Redis keys: `redis-cli -n 1 KEYS liq:*`
4. Review Jupyter notebook for data analysis examples

---

## ğŸš€ Production Deployment

### **Docker Deployment (Recommended)**

1. **Configure Environment:**
```bash
# Copy environment template
cp .env.example .env

# Edit with your actual API keys
nano .env
```

2. **Deploy with Docker Compose:**
```bash
# Build and start all services
./deploy.sh deploy

# Or using docker-compose directly
docker-compose up -d --build
```

3. **Check Status:**
```bash
# View running containers
./deploy.sh status

# View logs
./deploy.sh logs

# Health check
curl http://localhost:8080/health
```

4. **Stop Services:**
```bash
./deploy.sh stop
```

### **Manual Deployment**

1. **Install Dependencies:**
```bash
pip install -r requirements.txt
```

2. **Start Supporting Services:**
```bash
# Redis
redis-server --port 6379

# PostgreSQL + TimescaleDB
pg_ctl start

# Prometheus
prometheus --config.file=monitoring/prometheus_config.yml

# Grafana
grafana-server
```

3. **Run Application:**
```bash
python professional_liquidation_monitor.py
```

### **Monitoring**

- **Grafana Dashboard:** http://localhost:3000 (admin/password from .env)
- **Prometheus Metrics:** http://localhost:9090
- **Health Check:** http://localhost:8080/health
- **Application Logs:** `docker-compose logs -f liquidation-monitor`

### **File Structure**
```
liquidation-aggregator/
â”œâ”€â”€ deploy.sh                    # Production deployment script
â”œâ”€â”€ docker-compose.yml           # Docker orchestration
â”œâ”€â”€ Dockerfile                   # Container definition
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example                # Environment template
â”œâ”€â”€ config/
â”‚   â””â”€â”€ production.yaml         # Production configuration
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ prometheus_config.yml   # Prometheus settings
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ health_check.py         # Health check endpoint
â””â”€â”€ alerts/                     # Alert configurations
```

---

## ğŸ“„ License

Part of the Crypto Assistant project.

---

**Ready to track institutional liquidations in real-time! ğŸš€**
