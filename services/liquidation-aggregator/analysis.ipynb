{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Liquidation Data Analysis\n",
    "## Multi-Exchange Liquidation Tracking (Binance + Bybit, BTCUSDT)\n",
    "\n",
    "This notebook provides real-time and historical analysis of liquidation data across:\n",
    "- **Level 1:** In-Memory (last 60s)\n",
    "- **Level 2:** Redis (last 1h, aggregated)\n",
    "- **Level 3:** TimescaleDB (90 days, institutional events)\n",
    "- **Level 4:** TimescaleDB Continuous Aggregates (pre-computed rollups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import asyncio\n",
    "import asyncpg\n",
    "import redis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"âœ… Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Œ Connect to Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Redis (Level 2)\n",
    "redis_client = redis.Redis(host='localhost', port=6379, db=1, decode_responses=False)\n",
    "print(f\"âœ… Redis connected: {redis_client.ping()}\")\n",
    "\n",
    "# Connect to TimescaleDB (Level 3/4)\n",
    "async def get_db_connection():\n",
    "    return await asyncpg.connect(\n",
    "        host='localhost',\n",
    "        port=5432,\n",
    "        database='liquidations',\n",
    "        user='screener-m3'\n",
    "    )\n",
    "\n",
    "# Test connection\n",
    "conn = await get_db_connection()\n",
    "version = await conn.fetchval('SELECT version()')\n",
    "print(f\"âœ… Database connected: {version.split(',')[0]}\")\n",
    "await conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Level 2: Redis Analysis - Price Level Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all price level clusters from Redis\n",
    "def get_redis_price_levels(symbol='BTCUSDT'):\n",
    "    \"\"\"Get price level clusters from Redis\"\"\"\n",
    "    pattern = f\"liq:levels:{symbol}:*\"\n",
    "    clusters = []\n",
    "    \n",
    "    for key in redis_client.scan_iter(match=pattern):\n",
    "        data = redis_client.hgetall(key)\n",
    "        if not data:\n",
    "            continue\n",
    "        \n",
    "        # Parse key: liq:levels:BTCUSDT:67200:LONG\n",
    "        parts = key.decode().split(':')\n",
    "        price_level = float(parts[2])\n",
    "        side = parts[3]\n",
    "        \n",
    "        clusters.append({\n",
    "            'price_level': price_level,\n",
    "            'side': side,\n",
    "            'count': int(data.get(b'count', 0)),\n",
    "            'total_value': float(data.get(b'total_value', 0)),\n",
    "            'total_quantity': float(data.get(b'total_quantity', 0)),\n",
    "            'first_seen': int(data.get(b'first_seen', 0)),\n",
    "            'last_seen': int(data.get(b'last_seen', 0))\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(clusters)\n",
    "\n",
    "# Get price levels\n",
    "price_levels = get_redis_price_levels('BTCUSDT')\n",
    "print(f\"Found {len(price_levels)} price level clusters in Redis\")\n",
    "price_levels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price level heatmap\n",
    "if not price_levels.empty:\n",
    "    # Filter to top 20 levels by count\n",
    "    top_levels = price_levels.nlargest(20, 'count')\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for side in ['LONG', 'SHORT']:\n",
    "        side_data = top_levels[top_levels['side'] == side]\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=side_data['price_level'],\n",
    "            y=side_data['count'],\n",
    "            name=f'{side} Liquidations',\n",
    "            marker_color='red' if side == 'LONG' else 'green',\n",
    "            text=side_data['total_value'].apply(lambda x: f'${x/1e6:.2f}M'),\n",
    "            textposition='auto'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='BTCUSDT Liquidation Heatmap - Top 20 Price Levels (Last 1h)',\n",
    "        xaxis_title='Price Level',\n",
    "        yaxis_title='Number of Liquidations',\n",
    "        barmode='group',\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\nelse:\n    print(\"No price level data available yet. Start the aggregator and wait for data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Level 3: TimescaleDB Analysis - Significant Liquidations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recent significant liquidations from TimescaleDB\n",
    "async def get_significant_liquidations(hours=24, min_value=100_000):\n",
    "    \"\"\"Get significant liquidations from last N hours\"\"\"\n",
    "    conn = await get_db_connection()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        time,\n",
    "        exchange,\n",
    "        symbol,\n",
    "        side,\n",
    "        price,\n",
    "        quantity,\n",
    "        value_usd,\n",
    "        is_cascade,\n",
    "        risk_score\n",
    "    FROM liquidations_significant\n",
    "    WHERE time >= NOW() - INTERVAL '$1 hours'\n",
    "      AND value_usd >= $2\n",
    "    ORDER BY time DESC\n",
    "    LIMIT 1000\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = await conn.fetch(query, hours, min_value)\n",
    "    await conn.close()\n",
    "    \n",
    "    return pd.DataFrame([\n",
    "        {\n",
    "            'time': row['time'],\n",
    "            'exchange': row['exchange'],\n",
    "            'symbol': row['symbol'],\n",
    "            'side': row['side'],\n",
    "            'price': float(row['price']),\n",
    "            'quantity': float(row['quantity']),\n",
    "            'value_usd': float(row['value_usd']),\n",
    "            'is_cascade': row['is_cascade'],\n",
    "            'risk_score': float(row['risk_score']) if row['risk_score'] else None\n",
    "        }\n",
    "        for row in rows\n",
    "    ])\n",
    "\n",
    "# Get data\n",
    "significant_liqs = await get_significant_liquidations(hours=24)\n",
    "print(f\"Found {len(significant_liqs)} significant liquidations (>$100K) in last 24h\")\n",
    "significant_liqs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "if not significant_liqs.empty:\n",
    "    print(\"ðŸ“Š SUMMARY STATISTICS (Last 24h, >$100K liquidations)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Events: {len(significant_liqs)}\")\n",
    "    print(f\"Total Volume: ${significant_liqs['value_usd'].sum():,.0f}\")\n",
    "    print(f\"Average Size: ${significant_liqs['value_usd'].mean():,.0f}\")\n",
    "    print(f\"Largest: ${significant_liqs['value_usd'].max():,.0f}\")\n",
    "    print(f\"\")\n",
    "    print(\"Exchange Breakdown:\")\n",
    "    print(significant_liqs.groupby('exchange')['value_usd'].agg(['count', 'sum']).sort_values('sum', ascending=False))\n",
    "    print(f\"\")\n",
    "    print(\"Side Breakdown:\")\n",
    "    print(significant_liqs.groupby('side')['value_usd'].agg(['count', 'sum']))\n",
    "    print(f\"\")\n",
    "    print(f\"Cascades Detected: {significant_liqs['is_cascade'].sum()}\")\n",
    "    print(f\"Average Risk Score (cascades): {significant_liqs[significant_liqs['is_cascade']]['risk_score'].mean():.3f}\")\nelse:\n    print(\"No data yet. Start the aggregator and wait for liquidation events.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot\n",
    "if not significant_liqs.empty:\n",
    "    significant_liqs['hour'] = significant_liqs['time'].dt.floor('1H')\n",
    "    hourly = significant_liqs.groupby(['hour', 'exchange'])['value_usd'].sum().reset_index()\n",
    "    \n",
    "    fig = px.line(\n",
    "        hourly, \n",
    "        x='hour', \n",
    "        y='value_usd', \n",
    "        color='exchange',\n",
    "        title='Hourly Liquidation Volume by Exchange (Last 24h)',\n",
    "        labels={'value_usd': 'Total Volume (USD)', 'hour': 'Time'}\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Cascade Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cascade events\n",
    "async def get_cascades(hours=24):\n",
    "    \"\"\"Get cascade events from continuous aggregate\"\"\"\n",
    "    conn = await get_db_connection()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        cascade_id::text,\n",
    "        symbol,\n",
    "        cascade_start,\n",
    "        cascade_end,\n",
    "        duration_seconds,\n",
    "        event_count,\n",
    "        total_value_usd,\n",
    "        avg_risk_score,\n",
    "        max_risk_score,\n",
    "        exchanges_involved,\n",
    "        exchange_count,\n",
    "        long_count,\n",
    "        short_count\n",
    "    FROM liquidation_cascades\n",
    "    WHERE hour >= NOW() - INTERVAL '$1 hours'\n",
    "    ORDER BY cascade_start DESC\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = await conn.fetch(query, hours)\n",
    "    await conn.close()\n",
    "    \n",
    "    return pd.DataFrame([dict(row) for row in rows])\n",
    "\n",
    "# Get cascades\n",
    "cascades = await get_cascades(hours=24)\n",
    "print(f\"Found {len(cascades)} cascades in last 24h\")\n",
    "if not cascades.empty:\n",
    "    cascades.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cascade summary\n",
    "if not cascades.empty:\n",
    "    print(\"ðŸš¨ CASCADE SUMMARY (Last 24h)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Cascades: {len(cascades)}\")\n",
    "    print(f\"Cross-Exchange Cascades: {(cascades['exchange_count'] > 1).sum()}\")\n",
    "    print(f\"Total Cascade Volume: ${cascades['total_value_usd'].sum():,.0f}\")\n",
    "    print(f\"Average Cascade Size: ${cascades['total_value_usd'].mean():,.0f}\")\n",
    "    print(f\"Average Events per Cascade: {cascades['event_count'].mean():.1f}\")\n",
    "    print(f\"Average Duration: {cascades['duration_seconds'].mean():.1f}s\")\n",
    "    print(f\"Average Risk Score: {cascades['avg_risk_score'].mean():.3f}\")\n",
    "    print(f\"Max Risk Score: {cascades['max_risk_score'].max():.3f}\")\n",
    "    \n",
    "    # Top 5 cascades\n",
    "    print(f\"\\nTop 5 Largest Cascades:\")\n",
    "    print(cascades.nlargest(5, 'total_value_usd')[['symbol', 'event_count', 'total_value_usd', 'max_risk_score', 'exchange_count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Real-Time Monitoring - Check Current Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cascade status in Redis\n",
    "def check_cascade_status(symbol='BTCUSDT'):\n",
    "    \"\"\"Check if cascade is currently active\"\"\"\n",
    "    key = f\"liq:cascade:status:{symbol}\"\n",
    "    data = redis_client.hgetall(key)\n",
    "    \n",
    "    if not data:\n",
    "        return None\n",
    "    \n",
    "    return {k.decode(): v.decode() for k, v in data.items()}\n",
    "\n",
    "status = check_cascade_status('BTCUSDT')\n",
    "if status:\n",
    "    print(\"ðŸš¨ ACTIVE CASCADE DETECTED!\")\n",
    "    print(f\"Cascade ID: {status.get('cascade_id')}\")\n",
    "    print(f\"Event Count: {status.get('event_count')}\")\n",
    "    print(f\"Total Value: ${float(status.get('total_value', 0)):,.0f}\")\n",
    "    print(f\"Risk Score: {status.get('risk_score')}\")\n",
    "    print(f\"Exchanges: {status.get('exchanges')}\")\nelse:\n    print(\"âœ… No active cascade at the moment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Exchange Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get exchange comparison from continuous aggregate\n",
    "async def get_exchange_comparison(hours=24):\n",
    "    conn = await get_db_connection()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        period,\n",
    "        symbol,\n",
    "        exchange,\n",
    "        event_count,\n",
    "        total_value_usd,\n",
    "        avg_value_usd,\n",
    "        cascade_count,\n",
    "        long_count,\n",
    "        short_count\n",
    "    FROM liquidation_exchange_comparison\n",
    "    WHERE period >= NOW() - INTERVAL '$1 hours'\n",
    "    ORDER BY period DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = await conn.fetch(query, hours)\n",
    "    await conn.close()\n",
    "    \n",
    "    return pd.DataFrame([dict(row) for row in rows])\n",
    "\n",
    "# Get comparison\n",
    "comparison = await get_exchange_comparison(hours=24)\n",
    "if not comparison.empty:\n",
    "    print(f\"Exchange comparison data: {len(comparison)} periods\")\n",
    "    \n",
    "    # Aggregate by exchange\n",
    "    exchange_totals = comparison.groupby('exchange').agg({\n",
    "        'event_count': 'sum',\n",
    "        'total_value_usd': 'sum',\n",
    "        'avg_value_usd': 'mean',\n",
    "        'cascade_count': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(exchange_totals)\n",
    "else:\n",
    "    print(\"No comparison data yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Refresh this notebook periodically to see updated data\n",
    "\n",
    "Run all cells again to get the latest liquidation data and analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
